{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ['PYSPARK_PYTHON'] = sys.executable\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = sys.executable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LogisticRegression\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTAR FUNCIONES PROPIAS\n",
    "import sys\n",
    "sys.path.append('../../06.Big_data_PySpark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metricas\n",
    "from funciones import obtener_metricas_clasificacion\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load training data\n",
    "data = spark.read.format(\"libsvm\").load(\"../02.data/sample_libsvm_data.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN Y TEST**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .randomSplit()\n",
    "\n",
    "train, test = data.randomSplit(weights = [0.7, 0.3], seed = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INICIO MODELO VACIO\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model\n",
    "model = log_reg.fit(train)\n",
    "\n",
    "# Predicciones\n",
    "y_hat = model.evaluate(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(692,[100,101,102...|[3.65011628362891...|[0.97467016776878...|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[45.9467283021876...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[123,124,125...|[33.3618723288205...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[60.9473198030384...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[33.8179782963031...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[124,125,126...|[37.5584938791617...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[125,126,127...|[45.8622454395488...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[21.7144853715542...|[0.99999999962887...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[35.9974214525556...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[126,127,128...|[32.2118081448938...|[0.99999999999998...|       0.0|\n",
      "|  0.0|(692,[127,128,129...|[22.7821779838790...|[0.99999999987240...|       0.0|\n",
      "|  0.0|(692,[129,130,131...|[21.5299293443379...|[0.99999999955365...|       0.0|\n",
      "|  0.0|(692,[150,151,152...|[27.9868194994407...|[0.99999999999929...|       0.0|\n",
      "|  0.0|(692,[151,152,153...|[45.3246515765566...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[152,153,154...|[44.2974663854596...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[49.3516495965041...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[153,154,155...|[35.8307148202806...|[0.99999999999999...|       0.0|\n",
      "|  0.0|(692,[154,155,156...|[38.7488045071060...|           [1.0,0.0]|       0.0|\n",
      "|  0.0|(692,[234,235,237...|[0.55437921854799...|[0.63515100509907...|       0.0|\n",
      "|  1.0|(692,[97,98,99,12...|[-10.903426331750...|[1.83947598130555...|       1.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "y_hat.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = y_hat.predictions.select(\"label\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\anaconda3\\Lib\\site-packages\\pyspark\\sql\\context.py:158: FutureWarning: Deprecated in 3.0.0. Use SparkSession.builder.getOrCreate() instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Confusion Matrix': array([[19.,  1.],\n",
       "        [ 0., 15.]]),\n",
       " 'Precision label 0': 1.0,\n",
       " 'Precision label 1': 0.9375,\n",
       " 'Recall label 0': 0.95,\n",
       " 'Recall label 1': 1.0,\n",
       " 'F1-Score label 0': 0.9743589743589743,\n",
       " 'F1-Score label 1': 0.967741935483871,\n",
       " 'Accuracy': 0.9714285714285714,\n",
       " 'Falsos positivos label 0': 0.0,\n",
       " 'Falsos positivos label 1': 0.05}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obtener_metricas_clasificacion(y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_**Documentacion:** https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.mllib.evaluation.MulticlassMetrics.html_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluators == PREDICT\n",
    "\n",
    "**Documentacion:** Binary <br> https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.BinaryClassificationEvaluator\n",
    "\n",
    "**Documentacion:** MultiClass <br>\n",
    "https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.evaluation.MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Inicio objeto evaluador para problemas binarios\n",
    "evaluator = BinaryClassificationEvaluator(rawPredictionCol = \"prediction\", labelCol = \"label\")\n",
    "\n",
    "# No tiene accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96875"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sin parametros de metricas, ROC como defecto\n",
    "evaluator.evaluate(dataset = y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96875"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Con parametros\n",
    "evaluator.evaluate(dataset = y_hat, params = {evaluator.metricName: \"areaUnderROC\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830357142857142"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Area Under Precision-Recall, esta metrica puede resultar util cuando las clases estan desbalanceadas\n",
    "evaluator.evaluate(dataset = y_hat, params = {evaluator.metricName: \"areaUnderPR\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.96875"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MultiClass\n",
    "evaluator_m = MulticlassClassificationEvaluator(predictionCol = \"prediction\", labelCol = \"label\", metricName = \"accuracy\")\n",
    "\n",
    "accuracy = evaluator.evaluate(y_hat)\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################################################################################################################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
